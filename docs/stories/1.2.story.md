# Story 1.2: Database Schema Design and Migration System

## Status

**Approved**

## Story

**As a** developer,
**I want** the core database schema designed and migration system configured,
**so that** I can evolve the schema safely throughout development.

## Acceptance Criteria

1. Migration tool (golang-migrate) configured with up/down migrations
2. Initial migration creates core tables: `users`, `tenants`, `mailboxes`, `emails`, `attachments`, `jobs`, `audit_logs`
3. Users table includes: id (UUID), email, password_hash, role (MSP Admin/Tenant Admin/User), mfa_secret, created_at, updated_at
4. Tenants table includes: id (UUID), name, azure_tenant_id, azure_app_credentials (encrypted), retention_policy_days, legal_hold (boolean), created_at
5. Mailboxes table includes: id (UUID), tenant_id (FK), email_address, display_name, mailbox_type, sync_enabled, last_sync_at, created_at
6. Emails table includes: id (UUID), mailbox_id (FK), message_id, subject, sender, recipients, sent_at, body_text, body_html, has_attachments, size_bytes, indexed_at
7. Attachments table includes: id (UUID), email_id (FK), filename, content_type, size_bytes, sha256_hash, file_path
8. Jobs table includes: id (UUID), type, status, tenant_id (FK), mailbox_id (FK nullable), started_at, completed_at, error_message, metadata (JSONB)
9. Audit_logs table includes: id (UUID), user_id (FK), action, ip_address, timestamp, details (JSONB), immutable (enforced by trigger)
10. Foreign key constraints properly defined with ON DELETE CASCADE where appropriate
11. Indexes created on frequently queried columns (tenant_id, mailbox_id, email message_id, sent_at)
12. Migration runs successfully on fresh database with `make migrate-up` command
13. Down migration successfully rolls back all changes

## Tasks / Subtasks

- [ ] **Task 1: Install and configure golang-migrate tool** (AC: 1)
  - [ ] Install golang-migrate CLI tool in development environment
  - [ ] Create `/migrations` directory in repository root
  - [ ] Configure Makefile with `migrate-up` and `migrate-down` targets
  - [ ] Document migration tool usage in README or docs

- [ ] **Task 2: Create initial migration file with table definitions** (AC: 2, 3, 4, 5, 6, 7, 8, 9)
  - **Note:** Tasks 2-7 all add content to the same file (`000001_initial_schema.up.sql`) in proper execution order: extensions → tables → foreign keys → indexes → triggers → initial data
  - [ ] Create file `/migrations/000001_initial_schema.up.sql`
  - [ ] Add PostgreSQL extensions (uuid-ossp, pgcrypto)
  - [ ] Define Tenants table with all required fields and encrypted credentials storage (no dependencies)
  - [ ] Define Users table with all required fields and constraints (references Tenants)
  - [ ] Define Mailboxes table with tenant foreign key and unique constraint (references Tenants)
  - [ ] Define Emails table with mailbox foreign key and array field for recipients (references Mailboxes)
  - [ ] Define Attachments table with email foreign key and deduplication support (references Emails)
  - [ ] Define Jobs table with nullable foreign keys and JSONB metadata (references Tenants, Mailboxes, Users)
  - [ ] Define Audit_logs table with immutable constraint (references Users)
  - [ ] Define Settings table for global configuration (no dependencies)
  - [ ] Verify all table CREATE statements succeed without foreign key errors

- [ ] **Task 3: Add foreign key constraints with proper cascade rules** (AC: 10)
  - [ ] Users.tenant_id → Tenants.id (ON DELETE CASCADE)
  - [ ] Mailboxes.tenant_id → Tenants.id (ON DELETE CASCADE)
  - [ ] Emails.mailbox_id → Mailboxes.id (ON DELETE CASCADE)
  - [ ] Attachments.email_id → Emails.id (ON DELETE CASCADE)
  - [ ] Jobs.tenant_id → Tenants.id (ON DELETE CASCADE)
  - [ ] Jobs.mailbox_id → Mailboxes.id (ON DELETE CASCADE)
  - [ ] Jobs.user_id → Users.id (ON DELETE SET NULL)
  - [ ] Audit_logs.user_id → Users.id (ON DELETE SET NULL)

- [ ] **Task 4: Create indexes on frequently queried columns** (AC: 11)
  - [ ] Index on users.email (unique constraint)
  - [ ] Index on users.tenant_id
  - [ ] Index on tenants.azure_tenant_id
  - [ ] Index on mailboxes.tenant_id
  - [ ] Index on mailboxes.sync_enabled
  - [ ] Index on emails.mailbox_id
  - [ ] Index on emails.message_id (unique constraint)
  - [ ] Index on emails.sent_at (descending for recent emails)
  - [ ] Index on emails.sender
  - [ ] Index on emails.deleted_at (partial index WHERE deleted_at IS NULL)
  - [ ] Index on attachments.email_id
  - [ ] Index on attachments.sha256_hash (for deduplication)
  - [ ] Index on jobs.status, jobs.type, jobs.tenant_id, jobs.user_id, jobs.created_at
  - [ ] Index on audit_logs.user_id, audit_logs.action, audit_logs.timestamp

- [ ] **Task 5: Implement audit log immutability trigger** (AC: 9)
  - [ ] Create PostgreSQL function `prevent_audit_log_modification()`
  - [ ] Create trigger `audit_log_immutable_trigger` on UPDATE/DELETE
  - [ ] Verify trigger raises exception on modification attempts

- [ ] **Task 6: Implement updated_at trigger for users table** (AC: 3)
  - [ ] Create PostgreSQL function `update_updated_at_column()`
  - [ ] Create trigger `update_users_updated_at` on UPDATE
  - [ ] Verify trigger automatically updates timestamp on user modifications

- [ ] **Task 7: Add initial settings records**
  - [ ] Insert global_retention_policy_days setting (2555 days / ~7 years)
  - [ ] Insert smtp_config placeholder
  - [ ] Insert notification_channels configuration
  - [ ] Insert scheduler_enabled setting
  - [ ] Insert sync_schedule cron expression

- [ ] **Task 8: Create down migration file (000001_initial_schema.down.sql)** (AC: 13)
  - [ ] Create file `/migrations/000001_initial_schema.down.sql`
  - [ ] Add DROP TABLE statements in reverse dependency order
  - [ ] Drop triggers before dropping tables
  - [ ] Drop functions after dropping triggers
  - [ ] Drop extensions last

- [ ] **Task 9: Test migration up on fresh database** (AC: 12)
  - [ ] Ensure Docker PostgreSQL service is running
  - [ ] Run `make migrate-up` command
  - [ ] Verify all tables created successfully
  - [ ] Verify all indexes created
  - [ ] Verify all triggers created
  - [ ] Verify initial settings records inserted
  - [ ] Check for any migration errors in output

- [ ] **Task 10: Test migration down rollback** (AC: 13)
  - [ ] Run `make migrate-down` command
  - [ ] Verify all tables dropped successfully
  - [ ] Verify database returns to empty state
  - [ ] Test migration up again after rollback to ensure repeatability

- [ ] **Task 11: Write unit tests for migration validation**
  - [ ] Create test file to validate schema structure
  - [ ] Test that all expected tables exist
  - [ ] Test that all foreign keys are properly configured
  - [ ] Test that all indexes exist
  - [ ] Test audit log immutability trigger
  - [ ] Test updated_at trigger for users table

## Dev Notes

### Previous Story Insights

From Story 1.1 (if completed):
- Docker PostgreSQL service should already be running on port 5432
- DATABASE_URL environment variable should be configured in `.env`
- Backend configuration module should exist at `/backend/internal/config/config.go`
- pgx database driver should already be configured

### Migration Tool Configuration

**Tool:** golang-migrate
**Documentation:** https://github.com/golang-migrate/migrate

**Installation:**
```bash
# macOS
brew install golang-migrate

# Linux
curl -L https://github.com/golang-migrate/migrate/releases/download/v4.17.0/migrate.linux-amd64.tar.gz | tar xvz
sudo mv migrate /usr/local/bin/
```

**Migration File Naming Convention:**
- Format: `{version}_{description}.{up|down}.sql`
- Example: `000001_initial_schema.up.sql` and `000001_initial_schema.down.sql`

**Makefile Targets:**

Add these targets to root `Makefile` (created in Story 1.1):

```makefile
# Migration tool configuration
MIGRATE=migrate

# Load DATABASE_URL from environment or .env file
# If not set, use default for local development
ifndef DATABASE_URL
DATABASE_URL := postgres://ironarchive:password@localhost:5432/ironarchive?sslmode=disable
endif

migrate-up:
	@echo "Running migrations up..."
	$(MIGRATE) -path ./migrations -database "$(DATABASE_URL)" up

migrate-down:
	@echo "Rolling back migrations..."
	$(MIGRATE) -path ./migrations -database "$(DATABASE_URL)" down

migrate-create:
	@read -p "Enter migration name: " name; \
	$(MIGRATE) create -ext sql -dir ./migrations -seq $$name

migrate-status:
	$(MIGRATE) -path ./migrations -database "$(DATABASE_URL)" version
```

**Important Notes:**
- `DATABASE_URL` is read from environment variables (set in `.env` file)
- If not set, falls back to local development default
- Never commit real database credentials to version control
- Use `make migrate-status` to check current migration version

[Source: Common migration tool patterns]

### Database Schema Details

**Complete Schema Location:** The full database schema is documented in `docs/architecture/database-schema.md`

**Core Tables Overview:**

1. **Users Table** - System users with role-based access
2. **Tenants Table** - MSP customers with M365 tenant configuration
3. **Mailboxes Table** - M365 mailboxes configured for backup
4. **Emails Table** - Archived email messages with metadata
5. **Attachments Table** - Email attachments with deduplication
6. **Jobs Table** - Background job tracking (sync, export, retention)
7. **Audit_logs Table** - Immutable audit trail for compliance
8. **Settings Table** - Global configuration key-value store

[Source: architecture/database-schema.md#complete-sql-schema]

### Users Table Specification

```sql
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    display_name VARCHAR(255) NOT NULL,
    role VARCHAR(50) NOT NULL CHECK (role IN ('MSP_ADMIN', 'TENANT_ADMIN', 'USER')),
    tenant_id UUID REFERENCES tenants(id) ON DELETE CASCADE,
    mfa_secret VARCHAR(255),
    mfa_enabled BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_tenant_id ON users(tenant_id);
```

**Key Details:**
- UUID primary key with auto-generation
- Email is unique and used for authentication
- Password stored as bcrypt hash (cost factor 12+)
- Role enum: MSP_ADMIN, TENANT_ADMIN, USER
- MSP Admins have NULL tenant_id, others have required tenant_id
- MFA support with TOTP secret storage
- Auto-updating updated_at timestamp via trigger

[Source: architecture/database-schema.md#users-table]
[Source: architecture/data-models.md#user]

### Tenants Table Specification

```sql
CREATE TABLE tenants (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    azure_tenant_id UUID NOT NULL,
    azure_app_id UUID NOT NULL,
    azure_app_secret TEXT NOT NULL, -- Encrypted with pgcrypto
    retention_policy_days INTEGER DEFAULT 2555, -- ~7 years
    legal_hold BOOLEAN DEFAULT FALSE,
    whitelabel_config JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    storage_bytes BIGINT DEFAULT 0
);

CREATE INDEX idx_tenants_azure_tenant_id ON tenants(azure_tenant_id);
```

**Key Details:**
- Stores MSP customer information
- Azure AD credentials encrypted using pgcrypto extension
- Retention policy defaults to 7 years (2555 days)
- Legal hold prevents email deletion when enabled
- Whitelabel config stored as JSONB for custom branding
- Storage bytes tracked for billing/reporting

**Encryption Pattern:**
```sql
-- Insert with encryption
INSERT INTO tenants (name, azure_tenant_id, azure_app_id, azure_app_secret)
VALUES ($1, $2, $3, pgp_sym_encrypt($4, 'encryption_key_from_config'));

-- Query with decryption
SELECT id, name, pgp_sym_decrypt(azure_app_secret::bytea, 'encryption_key_from_config') as azure_app_secret
FROM tenants WHERE id = $1;
```

[Source: architecture/database-schema.md#tenants-table]
[Source: architecture/data-models.md#tenant]
[Source: architecture/backend-architecture.md#database-access-layer]

### Mailboxes Table Specification

```sql
CREATE TABLE mailboxes (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE,
    email_address VARCHAR(255) NOT NULL,
    display_name VARCHAR(255),
    mailbox_type VARCHAR(50) NOT NULL CHECK (mailbox_type IN ('USER', 'SHARED', 'ROOM', 'EQUIPMENT')),
    sync_enabled BOOLEAN DEFAULT FALSE,
    last_sync_at TIMESTAMP,
    last_delta_token TEXT,
    email_count INTEGER DEFAULT 0,
    storage_bytes BIGINT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(tenant_id, email_address)
);

CREATE INDEX idx_mailboxes_tenant_id ON mailboxes(tenant_id);
CREATE INDEX idx_mailboxes_sync_enabled ON mailboxes(sync_enabled);
```

**Key Details:**
- Each mailbox belongs to exactly one tenant
- Email address unique per tenant (composite unique constraint)
- Mailbox type enum supports User, Shared, Room, Equipment
- Delta token stores Microsoft Graph sync state for incremental sync
- Sync enabled flag controls whether mailbox is actively archived
- Computed metrics: email_count, storage_bytes

[Source: architecture/database-schema.md#mailboxes-table]
[Source: architecture/data-models.md#mailbox]

### Emails Table Specification

```sql
CREATE TABLE emails (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    mailbox_id UUID NOT NULL REFERENCES mailboxes(id) ON DELETE CASCADE,
    message_id VARCHAR(255) UNIQUE NOT NULL,
    subject TEXT,
    sender VARCHAR(255),
    recipients TEXT[], -- PostgreSQL array type
    sent_at TIMESTAMP NOT NULL,
    has_attachments BOOLEAN DEFAULT FALSE,
    size_bytes INTEGER NOT NULL,
    file_path TEXT NOT NULL, -- Filesystem path to email body JSON
    indexed_at TIMESTAMP,
    deleted_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_emails_mailbox_id ON emails(mailbox_id);
CREATE INDEX idx_emails_message_id ON emails(message_id);
CREATE INDEX idx_emails_sent_at ON emails(sent_at DESC);
CREATE INDEX idx_emails_sender ON emails(sender);
CREATE INDEX idx_emails_deleted_at ON emails(deleted_at) WHERE deleted_at IS NULL;
```

**Key Details:**
- message_id is Microsoft Graph message ID (globally unique)
- Recipients stored as PostgreSQL TEXT[] array (To, CC, BCC combined)
- File path points to JSON file on disk containing body_text and body_html
- Soft delete supported via deleted_at timestamp
- indexed_at tracks Meilisearch indexing status
- Descending index on sent_at for recent email queries
- Partial index on deleted_at for active email queries

[Source: architecture/database-schema.md#emails-table]
[Source: architecture/data-models.md#email]

### Attachments Table Specification

```sql
CREATE TABLE attachments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    email_id UUID NOT NULL REFERENCES emails(id) ON DELETE CASCADE,
    filename VARCHAR(255) NOT NULL,
    content_type VARCHAR(255),
    size_bytes INTEGER NOT NULL,
    sha256_hash VARCHAR(64) NOT NULL,
    file_path TEXT NOT NULL, -- Deduplicated by hash
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_attachments_email_id ON attachments(email_id);
CREATE INDEX idx_attachments_sha256_hash ON attachments(sha256_hash);
```

**Key Details:**
- Each attachment belongs to one email
- SHA-256 hash enables deduplication (same file = same storage location)
- File path derived from hash for deduplication
- Content type stores MIME type
- Multiple attachments can reference same file_path if hash matches

**Deduplication Pattern:**
- Calculate SHA-256 hash of attachment content
- Check if hash exists in attachments table
- If exists, reuse file_path; if not, store file and create new file_path
- File path format: `/data/attachments/{first_2_hash_chars}/{hash}.bin`

[Source: architecture/database-schema.md#attachments-table]
[Source: architecture/data-models.md#attachment]

### Jobs Table Specification

```sql
CREATE TABLE jobs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    type VARCHAR(50) NOT NULL CHECK (type IN ('SYNC_MAILBOX', 'SYNC_TENANT', 'SYNC_ALL', 'EXPORT', 'RETENTION_CLEANUP')),
    status VARCHAR(50) NOT NULL CHECK (status IN ('QUEUED', 'RUNNING', 'COMPLETED', 'FAILED')),
    tenant_id UUID REFERENCES tenants(id) ON DELETE CASCADE,
    mailbox_id UUID REFERENCES mailboxes(id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(id) ON DELETE SET NULL,
    progress INTEGER DEFAULT 0 CHECK (progress >= 0 AND progress <= 100),
    metadata JSONB,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_type ON jobs(type);
CREATE INDEX idx_jobs_tenant_id ON jobs(tenant_id);
CREATE INDEX idx_jobs_user_id ON jobs(user_id);
CREATE INDEX idx_jobs_created_at ON jobs(created_at DESC);
```

**Key Details:**
- Tracks background job execution (sync, export, retention)
- Type enum: SYNC_MAILBOX, SYNC_TENANT, SYNC_ALL, EXPORT, RETENTION_CLEANUP
- Status enum: QUEUED, RUNNING, COMPLETED, FAILED
- Nullable foreign keys for flexible job types
- Progress percentage (0-100) for UI display
- JSONB metadata stores job-specific data (export format, error details, etc.)
- Multiple indexes for efficient job queue queries

[Source: architecture/database-schema.md#jobs-table]
[Source: architecture/data-models.md#job]

### Audit_logs Table Specification

```sql
CREATE TABLE audit_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE SET NULL,
    action VARCHAR(100) NOT NULL,
    ip_address INET,
    details JSONB,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL
);

CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_action ON audit_logs(action);
CREATE INDEX idx_audit_logs_timestamp ON audit_logs(timestamp DESC);

-- Immutability trigger
CREATE OR REPLACE FUNCTION prevent_audit_log_modification()
RETURNS TRIGGER AS $$
BEGIN
    RAISE EXCEPTION 'Audit logs are immutable and cannot be modified or deleted';
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER audit_log_immutable_trigger
BEFORE UPDATE OR DELETE ON audit_logs
FOR EACH ROW EXECUTE FUNCTION prevent_audit_log_modification();
```

**Key Details:**
- Immutable audit trail for compliance (DSGVO/GoBD)
- Records user actions: LOGIN, SEARCH, EXPORT, CONFIG_CHANGE, etc.
- IP address stored as PostgreSQL INET type
- JSONB details store action-specific information
- Trigger prevents any UPDATE or DELETE operations
- User ID nullable to preserve audit log if user deleted

**Common Audit Actions:**
- `LOGIN` - User authentication
- `LOGOUT` - User logout
- `SEARCH` - Email search queries
- `EXPORT` - Email export operations
- `CONFIG_CHANGE` - Configuration modifications
- `TENANT_CREATE` - Tenant creation
- `MAILBOX_ADD` - Mailbox added
- `USER_CREATE` - User account creation

[Source: architecture/database-schema.md#audit-logs-table]
[Source: architecture/data-models.md#auditlog]

### Settings Table Specification

```sql
CREATE TABLE settings (
    key VARCHAR(255) PRIMARY KEY,
    value JSONB NOT NULL,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Initial settings
INSERT INTO settings (key, value) VALUES
('global_retention_policy_days', '2555'),
('smtp_config', '{}'),
('notification_channels', '{"email": true, "teams": false, "discord": false}'),
('scheduler_enabled', 'true'),
('sync_schedule', '"0 6,12,18,0 * * *"');
```

**Key Details:**
- Key-value store for global configuration
- Values stored as JSONB for flexible data types
- Initial settings provide defaults for application
- sync_schedule is cron expression (every 6 hours)

[Source: architecture/database-schema.md#settings-table]

### Foreign Key Cascade Rules

**ON DELETE CASCADE** (child records deleted with parent):
- users.tenant_id → tenants.id
- mailboxes.tenant_id → tenants.id
- emails.mailbox_id → mailboxes.id
- attachments.email_id → emails.id
- jobs.tenant_id → tenants.id
- jobs.mailbox_id → mailboxes.id

**ON DELETE SET NULL** (preserve child record, clear reference):
- jobs.user_id → users.id
- audit_logs.user_id → users.id

**Rationale:**
- CASCADE: When tenant/mailbox deleted, all associated data should be removed
- SET NULL: Jobs and audit logs should be preserved for historical record even if user deleted

[Source: architecture/database-schema.md#foreign-key-constraints]

### Database Connection Configuration

**Driver:** pgx (PostgreSQL driver for Go)
**Connection Pool:** pgxpool.Pool

**Connection String Format:**
```
postgres://username:password@host:port/database?sslmode=disable
```

**Environment Variable:** `DATABASE_URL`

[Source: architecture/tech-stack.md - Database row]
[Source: architecture/backend-architecture.md#service-architecture]

### Migration File Structure

```
/migrations/
├── 000001_initial_schema.up.sql       # Creates all tables, indexes, triggers
├── 000001_initial_schema.down.sql     # Drops all tables, indexes, triggers
├── 000002_future_migration.up.sql     # Future schema changes
└── 000002_future_migration.down.sql   # Rollback for future changes
```

**Up Migration Execution Order:**
1. Enable extensions (uuid-ossp, pgcrypto)
2. Create tables in dependency order (parent tables first)
3. Create indexes after tables
4. Create trigger functions
5. Create triggers
6. Insert initial data

**Down Migration Execution Order (reverse):**
1. Drop triggers
2. Drop functions
3. Drop tables in reverse dependency order (child tables first)
4. Drop extensions

[Source: Common migration best practices]

### Project Structure Notes

Migration files location: `/migrations` (repository root)

This aligns with the project structure defined in Story 1.1 and ensures migration files are version controlled alongside code.

**No conflicts found** between epic requirements and architecture specifications.

### Troubleshooting Common Migration Issues

**Problem: Migration fails with "relation does not exist" error**
- **Cause**: Tables referenced in foreign keys don't exist yet
- **Solution**: Verify table creation order follows dependency chain (Tenants → Users → Mailboxes → Emails → Attachments → Jobs → Audit_logs)
- **Check**: Review Task 2 subtasks for correct order

**Problem: Migration fails with "database does not exist"**
- **Cause**: PostgreSQL database not created
- **Solution**:
  ```bash
  # Connect to PostgreSQL and create database
  docker exec -it <postgres-container> psql -U ironarchive -c "CREATE DATABASE ironarchive;"
  # Or use createdb command
  docker exec -it <postgres-container> createdb -U ironarchive ironarchive
  ```

**Problem: Migration fails with "connection refused"**
- **Cause**: PostgreSQL service not running or DATABASE_URL incorrect
- **Solution**:
  - Check Docker services: `docker ps` (verify postgres container running)
  - Check DATABASE_URL in `.env` file matches Docker service configuration
  - Test connection: `docker exec -it <postgres-container> psql -U ironarchive -d ironarchive`

**Problem: Migration fails with "no migration"**
- **Cause**: Migration files not found or path incorrect
- **Solution**:
  - Verify `/migrations` directory exists in repository root
  - Check migration files exist: `ls migrations/`
  - Verify Makefile `-path` flag points to `./migrations`

**Problem: Migration succeeds but tables missing**
- **Cause**: Wrong database selected
- **Solution**: Check DATABASE_URL database name matches intended database
- **Check**: `\c ironarchive` in psql to connect to correct database

**Problem: Trigger creation fails**
- **Cause**: Function not created before trigger, or syntax error
- **Solution**:
  - Verify function created before trigger in migration file
  - Check function syntax with `\df function_name` in psql
  - Review trigger syntax in Dev Notes (lines 420-430)

**Problem: Down migration fails with "dependent objects"**
- **Cause**: Objects depend on items being dropped
- **Solution**: Drop in correct order (triggers → functions → tables → extensions)
- **Check**: Review down migration execution order (lines 551-556)

**Debugging Commands:**
```bash
# Check migration version
make migrate-status

# Check if tables exist
docker exec -it <postgres-container> psql -U ironarchive -d ironarchive -c "\dt"

# Check specific table structure
docker exec -it <postgres-container> psql -U ironarchive -d ironarchive -c "\d users"

# Check for errors in PostgreSQL logs
docker logs <postgres-container>

# Force migration version (use with caution)
migrate -path ./migrations -database "$DATABASE_URL" force <version>
```

## Testing

### Testing Standards for This Story

**Backend Testing Framework:**
- Go Testing (standard library)
- Testify 1.9+ for assertions and test helpers

[Source: architecture/tech-stack.md - Backend Testing row]

**Test Organization:**

For this story, focus on **migration validation and database schema tests**:

1. **Migration Execution Tests:**
   - Test migration up completes without errors
   - Test migration down completes without errors
   - Test migration up after down (repeatability)

2. **Schema Validation Tests:**
   - Verify all expected tables exist
   - Verify all columns exist with correct data types
   - Verify all foreign key constraints exist
   - Verify all indexes exist
   - Verify all triggers exist

3. **Trigger Behavior Tests:**
   - Test audit log immutability (UPDATE/DELETE should fail)
   - Test users.updated_at auto-update on modification

4. **Data Integrity Tests:**
   - Test foreign key CASCADE behavior
   - Test foreign key SET NULL behavior
   - Test CHECK constraints (role enum, status enum, progress range)
   - Test UNIQUE constraints (email, message_id, tenant_id+email_address)

**Test File Location:**

```
/backend/internal/database/
├── migrations_test.go           # Migration execution tests
└── schema_test.go               # Schema validation tests
```

[Source: architecture/testing-strategy.md#test-organization]

**Example Test Structure:**

```go
// backend/internal/database/schema_test.go
package database_test

import (
    "testing"
    "github.com/stretchr/testify/assert"
)

func TestSchemaTablesExist(t *testing.T) {
    db := setupTestDatabase(t)
    defer teardownTestDatabase(t, db)

    tables := []string{"users", "tenants", "mailboxes", "emails", "attachments", "jobs", "audit_logs", "settings"}

    for _, table := range tables {
        var exists bool
        err := db.QueryRow("SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = $1)", table).Scan(&exists)
        assert.NoError(t, err)
        assert.True(t, exists, "Table %s should exist", table)
    }
}

func TestAuditLogImmutability(t *testing.T) {
    db := setupTestDatabase(t)
    defer teardownTestDatabase(t, db)

    // Insert audit log
    var logID string
    err := db.QueryRow("INSERT INTO audit_logs (action, ip_address) VALUES ($1, $2) RETURNING id", "TEST_ACTION", "127.0.0.1").Scan(&logID)
    assert.NoError(t, err)

    // Attempt to update (should fail)
    _, err = db.Exec("UPDATE audit_logs SET action = $1 WHERE id = $2", "MODIFIED", logID)
    assert.Error(t, err)
    assert.Contains(t, err.Error(), "Audit logs are immutable")

    // Attempt to delete (should fail)
    _, err = db.Exec("DELETE FROM audit_logs WHERE id = $1", logID)
    assert.Error(t, err)
    assert.Contains(t, err.Error(), "Audit logs are immutable")
}
```

[Source: architecture/testing-strategy.md#backend-api-test]

**Manual Verification Steps:**

1. Run `make migrate-up` and verify no errors
2. Connect to database and run `\dt` to list tables
3. Run `\d table_name` to inspect table structure
4. Run `make migrate-down` and verify clean rollback
5. Run `make migrate-up` again to verify repeatability

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-17 | 1.0 | Initial story creation for Epic 1 Story 2 | Bob (Scrum Master) |
| 2025-10-17 | 1.1 | Added clarifying note to Task 2 explaining Tasks 2-7 contribute to same migration file | Sarah (Product Owner) |
| 2025-10-17 | 1.2 | **Critical improvements for 10/10 readiness:** Fixed table creation order in Task 2 to respect foreign key dependencies (Tenants first); Added verification subtask; Enhanced Makefile with environment variable loading and migrate-status command; Added comprehensive troubleshooting section with common issues and debugging commands | Sarah (Product Owner) |
| 2025-10-17 | 1.2 | Story approved for implementation - 10/10 implementation readiness achieved | Sarah (Product Owner) |

## Dev Agent Record

### Agent Model Used

_To be populated by Dev Agent during implementation_

### Debug Log References

_To be populated by Dev Agent during implementation_

### Completion Notes List

_To be populated by Dev Agent during implementation_

### File List

_To be populated by Dev Agent during implementation_

## QA Results

_To be populated by QA Agent after story completion_
