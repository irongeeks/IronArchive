# Story 1.1 - Path to 100/100 Quality Score

**Current Score: 95/100 PASS**
**Target Score: 100/100 EXCELLENT**

This guide provides step-by-step instructions to address the remaining LOW priority improvements and achieve perfect quality scores across all dimensions.

---

## Overview

All HIGH and MEDIUM priority items have been resolved. Three LOW priority optimizations remain:

| Issue ID | Priority | Effort | Impact Area |
|----------|----------|--------|-------------|
| PERF-001 | LOW | 2 hours | Performance (85→95) |
| CONFIG-001 | LOW | 1 hour | Maintainability (90→95) |
| INFRA-001 | LOW | 30 min | Reliability (80→90) |

**Total Estimated Effort: 3.5 hours**

---

## Issue 1: Connection Pool Configuration (PERF-001)

**Impact**: Performance: 85/100 → 95/100
**Effort**: 2 hours
**Priority**: LOW (optimize for production tuning)

### Problem

PostgreSQL connection pool uses pgxpool defaults without externalized configuration. This prevents runtime tuning based on actual load characteristics.

### Solution

Add connection pool configuration to the Config struct and apply to pgxpool.

### Step-by-Step Instructions

#### 1. Update `backend/internal/config/config.go`

Add connection pool fields to the Config struct:

```go
// Config holds all application configuration
type Config struct {
	DatabaseURL      string
	RedisURL         string
	MeilisearchURL   string
	MeiliMasterKey   string
	JWTSecret        string
	ServerPort       string
	ServerHost       string
	LogLevel         string
	LogFormat        string
	EmailStoragePath string

	// Database connection pool configuration
	DBMaxConns          int32
	DBMinConns          int32
	DBMaxConnLifetime   time.Duration
	DBMaxConnIdleTime   time.Duration
	DBHealthCheckPeriod time.Duration
}
```

Add this import at the top of the file:
```go
import (
	"fmt"
	"os"
	"strconv"
	"time"

	"github.com/joho/godotenv"
)
```

Update the `Load()` function to parse pool configuration:

```go
func Load() (*Config, error) {
	// Load .env file if it exists (optional for development)
	_ = godotenv.Load()

	cfg := &Config{
		DatabaseURL:      getEnv("DATABASE_URL", ""),
		RedisURL:         getEnv("REDIS_URL", ""),
		MeilisearchURL:   getEnv("MEILISEARCH_URL", ""),
		MeiliMasterKey:   getEnv("MEILI_MASTER_KEY", ""),
		JWTSecret:        getEnv("JWT_SECRET", ""),
		ServerPort:       getEnv("SERVER_PORT", "8080"),
		ServerHost:       getEnv("SERVER_HOST", "localhost"),
		LogLevel:         getEnv("LOG_LEVEL", "info"),
		LogFormat:        getEnv("LOG_FORMAT", "json"),
		EmailStoragePath: getEnv("EMAIL_STORAGE_PATH", "./data/emails"),

		// Database pool configuration with sensible defaults
		DBMaxConns:          getEnvAsInt32("DB_MAX_CONNS", 25),
		DBMinConns:          getEnvAsInt32("DB_MIN_CONNS", 5),
		DBMaxConnLifetime:   getEnvAsDuration("DB_MAX_CONN_LIFETIME", 1*time.Hour),
		DBMaxConnIdleTime:   getEnvAsDuration("DB_MAX_CONN_IDLE_TIME", 30*time.Minute),
		DBHealthCheckPeriod: getEnvAsDuration("DB_HEALTH_CHECK_PERIOD", 1*time.Minute),
	}

	// Validate required configuration
	if cfg.DatabaseURL == "" {
		return nil, fmt.Errorf("DATABASE_URL is required")
	}
	if cfg.RedisURL == "" {
		return nil, fmt.Errorf("REDIS_URL is required")
	}
	if cfg.MeilisearchURL == "" {
		return nil, fmt.Errorf("MEILISEARCH_URL is required")
	}
	if cfg.MeiliMasterKey == "" {
		return nil, fmt.Errorf("MEILI_MASTER_KEY is required")
	}
	if cfg.JWTSecret == "" {
		return nil, fmt.Errorf("JWT_SECRET is required")
	}

	return cfg, nil
}

// getEnvAsInt32 retrieves an environment variable as int32 or returns a default value
func getEnvAsInt32(key string, defaultValue int32) int32 {
	valueStr := os.Getenv(key)
	if valueStr == "" {
		return defaultValue
	}
	value, err := strconv.ParseInt(valueStr, 10, 32)
	if err != nil {
		return defaultValue
	}
	return int32(value)
}

// getEnvAsDuration retrieves an environment variable as time.Duration or returns a default value
func getEnvAsDuration(key string, defaultValue time.Duration) time.Duration {
	valueStr := os.Getenv(key)
	if valueStr == "" {
		return defaultValue
	}
	value, err := time.ParseDuration(valueStr)
	if err != nil {
		return defaultValue
	}
	return value
}
```

#### 2. Update `backend/internal/database/postgres.go`

Apply the connection pool configuration:

```go
package database

import (
	"context"
	"fmt"

	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"

	"ironarchive/internal/config"
)

// PostgresConnection manages PostgreSQL database connection
type PostgresConnection struct {
	Pool   *pgxpool.Pool
	logger *zap.Logger
}

// NewPostgresConnection creates a new PostgreSQL connection with pool configuration
func NewPostgresConnection(cfg *config.Config, logger *zap.Logger) (*PostgresConnection, error) {
	ctx := context.Background()

	// Parse connection URL
	poolConfig, err := pgxpool.ParseConfig(cfg.DatabaseURL)
	if err != nil {
		return nil, fmt.Errorf("failed to parse database URL: %w", err)
	}

	// Apply connection pool configuration
	poolConfig.MaxConns = cfg.DBMaxConns
	poolConfig.MinConns = cfg.DBMinConns
	poolConfig.MaxConnLifetime = cfg.DBMaxConnLifetime
	poolConfig.MaxConnIdleTime = cfg.DBMaxConnIdleTime
	poolConfig.HealthCheckPeriod = cfg.DBHealthCheckPeriod

	// Create connection pool
	pool, err := pgxpool.NewWithConfig(ctx, poolConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create connection pool: %w", err)
	}

	logger.Info("PostgreSQL connection pool configured",
		zap.Int32("max_conns", cfg.DBMaxConns),
		zap.Int32("min_conns", cfg.DBMinConns),
		zap.Duration("max_conn_lifetime", cfg.DBMaxConnLifetime),
		zap.Duration("max_conn_idle_time", cfg.DBMaxConnIdleTime),
	)

	return &PostgresConnection{
		Pool:   pool,
		logger: logger,
	}, nil
}

// Ping validates the database connection
func (p *PostgresConnection) Ping(ctx context.Context) error {
	if err := p.Pool.Ping(ctx); err != nil {
		return fmt.Errorf("failed to ping database: %w", err)
	}
	return nil
}

// Close closes the database connection pool
func (p *PostgresConnection) Close() {
	if p.Pool != nil {
		p.Pool.Close()
		p.logger.Info("PostgreSQL connection pool closed")
	}
}
```

#### 3. Update `backend/cmd/server/main.go`

Change the PostgreSQL connection call to pass the config:

```go
// Initialize PostgreSQL connection
logger.Info("Connecting to PostgreSQL...", zap.String("url", maskConnectionString(cfg.DatabaseURL)))
pgConn, err := database.NewPostgresConnection(cfg, logger)  // Pass cfg instead of just URL
if err != nil {
	logger.Error("Failed to create PostgreSQL connection", zap.Error(err))
	os.Exit(1)
}
defer pgConn.Close()
```

#### 4. Update `.env.example`

Add connection pool configuration documentation:

```bash
# Database Connection Pool Configuration
DB_MAX_CONNS=25                       # Maximum number of connections in the pool (default: 25)
DB_MIN_CONNS=5                        # Minimum number of connections maintained (default: 5)
DB_MAX_CONN_LIFETIME=1h               # Maximum lifetime of a connection (default: 1h)
DB_MAX_CONN_IDLE_TIME=30m             # Maximum time a connection can be idle (default: 30m)
DB_HEALTH_CHECK_PERIOD=1m             # How often to health check idle connections (default: 1m)
```

#### 5. Update Documentation

Add to `docs/architecture/backend-architecture.md` or create a section in README:

```markdown
### Database Connection Pool Tuning

IronArchive uses configurable connection pooling for PostgreSQL:

- **DB_MAX_CONNS**: Maximum concurrent connections (default: 25)
- **DB_MIN_CONNS**: Minimum maintained connections (default: 5)
- **DB_MAX_CONN_LIFETIME**: Connection lifetime before refresh (default: 1h)
- **DB_MAX_CONN_IDLE_TIME**: Idle timeout (default: 30m)

Tune based on your workload:
- **Light load** (< 10 concurrent users): MaxConns=10, MinConns=2
- **Medium load** (10-50 users): MaxConns=25, MinConns=5 (default)
- **Heavy load** (50+ users): MaxConns=50, MinConns=10
```

#### 6. Test

```bash
cd backend
go mod tidy
go build ./cmd/server
./server  # Verify pool configuration logs appear
```

---

## Issue 2: Externalize Timeout Configuration (CONFIG-001)

**Impact**: Maintainability: 90/100 → 95/100
**Effort**: 1 hour
**Priority**: LOW (improves configurability)

### Problem

Timeout values are hardcoded in `main.go` (5s for Meilisearch). Should be externalized to Config for runtime tuning.

### Solution

Add timeout configuration fields and apply them.

### Step-by-Step Instructions

#### 1. Update `backend/internal/config/config.go`

Add timeout fields to Config struct:

```go
type Config struct {
	// ... existing fields ...

	// Timeout configuration
	MeilisearchTimeout time.Duration
	ShutdownTimeout    time.Duration
}
```

Update `Load()` function:

```go
cfg := &Config{
	// ... existing fields ...

	// Timeout configuration
	MeilisearchTimeout: getEnvAsDuration("MEILISEARCH_TIMEOUT", 5*time.Second),
	ShutdownTimeout:    getEnvAsDuration("SHUTDOWN_TIMEOUT", 10*time.Second),

	// ... pool configuration ...
}
```

#### 2. Update `backend/cmd/server/main.go`

Replace hardcoded timeouts:

```go
// Before:
// pingCtx, pingCancel := context.WithTimeout(ctx, 5*time.Second)

// After:
pingCtx, pingCancel := context.WithTimeout(ctx, cfg.MeilisearchTimeout)
defer pingCancel()
if err := meiliConn.Ping(pingCtx); err != nil {
	logger.Error("Failed to ping Meilisearch", zap.Error(err))
	os.Exit(1)
}
```

```go
// Before:
// shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), 10*time.Second)

// After:
shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), cfg.ShutdownTimeout)
defer shutdownCancel()
```

#### 3. Update `.env.example`

```bash
# Timeout Configuration
MEILISEARCH_TIMEOUT=5s                # Meilisearch health check timeout (default: 5s)
SHUTDOWN_TIMEOUT=10s                  # Graceful shutdown timeout (default: 10s)
```

#### 4. Test

```bash
cd backend
go build ./cmd/server
MEILISEARCH_TIMEOUT=10s ./server  # Verify custom timeout works
```

---

## Issue 3: Docker Resource Limits (INFRA-001)

**Impact**: Reliability: 80/100 → 90/100
**Effort**: 30 minutes
**Priority**: LOW (prevents resource exhaustion)

### Problem

Docker services lack CPU and memory resource limits. This could allow runaway processes to consume all system resources.

### Solution

Add resource constraints to all services in `docker-compose.yml`.

### Step-by-Step Instructions

#### 1. Update `docker/docker-compose.yml`

Add resource limits to each service:

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:16-alpine
    container_name: ironarchive-postgres
    environment:
      POSTGRES_DB: ironarchive
      POSTGRES_USER: ironarchive
      POSTGRES_PASSWORD: ironarchive_password
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=en_US.UTF-8"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ironarchive -d ironarchive"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - ironarchive-network
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'          # Max 1 CPU core
          memory: 1G           # Max 1GB RAM
        reservations:
          cpus: '0.25'         # Reserve 0.25 cores
          memory: 256M         # Reserve 256MB

  redis:
    image: redis:7-alpine
    container_name: ironarchive-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    networks:
      - ironarchive-network
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'          # Max 0.5 CPU cores
          memory: 512M         # Max 512MB RAM
        reservations:
          cpus: '0.1'          # Reserve 0.1 cores
          memory: 128M         # Reserve 128MB

  meilisearch:
    image: getmeili/meilisearch:v1.6
    container_name: ironarchive-meilisearch
    environment:
      MEILI_MASTER_KEY: development_master_key_change_in_production
      MEILI_NO_ANALYTICS: "true"
      MEILI_ENV: development
    ports:
      - "7700:7700"
    volumes:
      - meilisearch_data:/meili_data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7700/health || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    networks:
      - ironarchive-network
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'          # Max 1 CPU core
          memory: 1G           # Max 1GB RAM
        reservations:
          cpus: '0.25'         # Reserve 0.25 cores
          memory: 256M         # Reserve 256MB

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  meilisearch_data:
    driver: local

networks:
  ironarchive-network:
    driver: bridge
```

#### 2. Add Resource Limits Documentation

Create `docker/README.md`:

```markdown
# Docker Configuration

## Resource Limits

Resource limits prevent runaway processes from consuming all system resources.

### Current Limits (Development)

| Service | CPU Limit | Memory Limit | CPU Reserved | Memory Reserved |
|---------|-----------|--------------|--------------|-----------------|
| PostgreSQL | 1.0 cores | 1GB | 0.25 cores | 256MB |
| Redis | 0.5 cores | 512MB | 0.1 cores | 128MB |
| Meilisearch | 1.0 cores | 1GB | 0.25 cores | 256MB |

### Production Tuning

For production deployments, adjust limits based on workload:

**PostgreSQL**:
- Light: 2 cores, 2GB
- Medium: 4 cores, 4GB
- Heavy: 8 cores, 8GB

**Redis**:
- Light: 0.5 cores, 512MB
- Medium: 1 core, 1GB
- Heavy: 2 cores, 2GB

**Meilisearch**:
- Light: 1 core, 1GB
- Medium: 2 cores, 2GB
- Heavy: 4 cores, 4GB
```

#### 3. Test

```bash
cd docker
docker-compose down
docker-compose up -d
docker stats  # Verify resource limits are applied
```

Expected output should show limits:
```
CONTAINER ID   NAME                      CPU %   MEM USAGE / LIMIT
abc123         ironarchive-postgres      0.5%    50MB / 1GB
def456         ironarchive-redis         0.2%    30MB / 512MB
ghi789         ironarchive-meilisearch   0.3%    100MB / 1GB
```

---

## Final Verification

After completing all three improvements, verify the changes:

### 1. Build and Test Backend

```bash
cd backend
go mod tidy
go test ./...
go build ./cmd/server
./server
```

Verify logs show:
- ✓ PostgreSQL pool configuration logged
- ✓ Configurable timeouts working
- ✓ All services start successfully

### 2. Test Docker Services

```bash
cd docker
docker-compose down -v
docker-compose up -d
docker stats
```

Verify:
- ✓ All health checks passing
- ✓ Resource limits visible in `docker stats`
- ✓ Services respond on expected ports

### 3. Update Story File List

Add to the "File List" section in `docs/stories/1.1.story.md`:

```markdown
**Modified (Optimization Pass)**:
- `/backend/internal/config/config.go` - Added connection pool and timeout configuration
- `/backend/internal/database/postgres.go` - Applied pool configuration to pgxpool
- `/backend/cmd/server/main.go` - Use configurable timeouts instead of hardcoded values
- `/docker/docker-compose.yml` - Added resource limits to all services
- `/.env.example` - Added pool and timeout configuration documentation
- `/docker/README.md` - Created (resource limits documentation)
```

---

## Expected Quality Score Improvements

After completing all improvements:

| Dimension | Before | After | Improvement |
|-----------|--------|-------|-------------|
| **Overall** | 95/100 | **100/100** | +5 |
| Security | 90/100 | 90/100 | - |
| **Performance** | 85/100 | **95/100** | +10 |
| **Reliability** | 80/100 | **90/100** | +10 |
| **Maintainability** | 90/100 | **95/100** | +5 |
| Testing | 90/100 | 90/100 | - |

**Gate Status**: PASS → **EXCELLENT**

---

## Optional: Advanced Optimizations

These are beyond the scope of this story but can be considered for future work:

### 1. Connection Retry Logic (4 hours)

Add exponential backoff for transient connection failures:

```go
func connectWithRetry(ctx context.Context, cfg *config.Config, logger *zap.Logger) (*pgxpool.Pool, error) {
	maxRetries := 5
	backoff := 1 * time.Second

	for i := 0; i < maxRetries; i++ {
		pool, err := pgxpool.NewWithConfig(ctx, poolConfig)
		if err == nil {
			return pool, nil
		}

		logger.Warn("Connection failed, retrying...",
			zap.Int("attempt", i+1),
			zap.Duration("backoff", backoff),
		)

		time.Sleep(backoff)
		backoff *= 2  // Exponential backoff
	}

	return nil, fmt.Errorf("failed after %d retries", maxRetries)
}
```

### 2. Metrics and Monitoring (8 hours)

Add Prometheus metrics for observability:
- Connection pool utilization
- Query latency percentiles
- Error rates
- Health check status

### 3. Configuration Hot Reload (6 hours)

Support configuration updates without restart using file watchers.

---

## Summary

These three optimizations (3.5 hours total) will bring Story 1.1 from **95/100 PASS** to **100/100 EXCELLENT**, making it a reference implementation for future scaffolding projects.

All improvements follow production best practices and provide runtime tunability for different deployment environments.

**Questions?** Contact Quinn (QA) or refer to the quality gate file at `docs/qa/gates/1.1-project-scaffolding.yml`.
